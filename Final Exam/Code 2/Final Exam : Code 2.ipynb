{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Exam : Code 2",
      "provenance": [],
      "mount_file_id": "1jWlsVZBO4UzErIwmO8NMvcYBvSRCNq_z",
      "authorship_tag": "ABX9TyML35v2xy4FDt5cXMpaxtdi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muchlisam17/Machine-Learning-Task-TelU/blob/main/Final%20Exam/Code%202/Final%20Exam%20%3A%20Code%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GD7PTT1lJdcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21c7682-89e8-40f2-b01d-f238a2e26329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Twitter-Sentiment-Analysis-main.zip\n",
            "01dd799404a4cbff4238a3f1eb12258828c16760\n",
            "   creating: Twitter-Sentiment-Analysis-main/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/.gitignore  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/LICENSE  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/README.md  \n",
            "   creating: Twitter-Sentiment-Analysis-main/code/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/baseline.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/cnn-feats-svm.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/cnn.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/decisiontree.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/extract-cnn-feats.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/logistic.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/lstm.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/majority-voting.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/maxent-nltk.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/naivebayes.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/neuralnet.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/preprocess.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/randomforest.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/stats.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/svm.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/utils.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/code/xgboost.py  \n",
            "   creating: Twitter-Sentiment-Analysis-main/dataset/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/dataset/negative-words.txt  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/dataset/positive-words.txt  \n",
            "   creating: Twitter-Sentiment-Analysis-main/docs/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/docs/Plots.ipynb  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/docs/Twitter Sentiment Analysis- (3).pdf  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/Twitter-Sentiment-Analysis-main.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip2 install numpy\n",
        "!pip2 install scikit-learn\n",
        "!pip2 install scipy\n",
        "!pip2 install nltk\n",
        "!pip2 install tensorflow\n",
        "!pip2 install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfI5cnUETdrN",
        "outputId": "a2bd2c0a-c879-46af-97f4-79a3dfb780d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (1.16.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (0.20.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python2.7/dist-packages (from scikit-learn) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scikit-learn) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scipy) (1.16.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python2.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python2.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: scipy==1.2.2; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.2.2)\n",
            "Requirement already satisfied: wheel; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.post1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: functools32>=3.2.3; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.2.3.post2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.3.2)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.10)\n",
            "Requirement already satisfied: mock>=2.0.0; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.8.0->tensorflow) (44.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.15.5)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.5)\n",
            "Requirement already satisfied: rsa<4.6; python_version < \"3.6\" in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.6.16)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.5)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python2.7/dist-packages (0.82)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (from xgboost) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from xgboost) (1.16.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocess"
      ],
      "metadata": {
        "id": "gnF6P6xKasTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/Twitter-Sentiment-Analysis-main/dataset/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbkexZ2OUagn",
        "outputId": "16ce5c55-1e5e-4e66-bd21-709fd3cc170f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: /content/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocess Test"
      ],
      "metadata": {
        "id": "x57g8n7IbCze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/Twitter-Sentiment-Analysis-main/dataset/test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79xSGxT9Y37X",
        "outputId": "06972318-0c71-4f92-ee08-cdfe01471e1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 300000/300000\n",
            "Saved processed tweets to: /content/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocess Running of stats.py"
      ],
      "metadata": {
        "id": "Z-6nUt0cbBDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/stats.py /content/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7j53NyGbAwu",
        "outputId": "80e19b5b-5599-4f48-adc7-183c87395f03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to /content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to /content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 88955, Avg: 0.8895, Max: 12\n",
            "URLs => Total: 3599, Avg: 0.0360, Max: 4\n",
            "Emojis => Total: 1184, Positive: 997, Negative: 187, Avg: 0.0118, Max: 5\n",
            "Words => Total: 1192617, Unique: 50377, Avg: 11.9262, Max: 40, Min: 0\n",
            "Bigrams => Total: 1093149, Unique: 385105, Avg: 10.9315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##baseline.py"
      ],
      "metadata": {
        "id": "3EKdweOMbfz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/baseline.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofM_CdCpb9tC",
        "outputId": "d0d90477-b07f-497f-f4d0-9308a2e7f63f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##naivebayes.py"
      ],
      "metadata": {
        "id": "-tJX8YIhcpHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/naivebayes.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw-Bs6PVdA7i",
        "outputId": "2c1c405d-3c11-4e09-edcd-d90aba261d50"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7821/10000 = 78.2100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##logistic.py"
      ],
      "metadata": {
        "id": "6R5hsYITgN6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/logistic.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqbqcVs5gN6j",
        "outputId": "ed206cfa-8914-4b56-d64d-43b86dfd6347"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-01-24 14:53:07.158961: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2022-01-24 14:53:07.160018: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2022-01-24 14:53:07.160063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Twitter-Sentiment-Analysis-main/code/logistic.py\", line 1, in <module>\n",
            "    from keras.models import Sequential, load_model\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/keras/__init__.py\", line 22, in <module>\n",
            "    from keras import distribute\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/keras/distribute/__init__.py\", line 18, in <module>\n",
            "    from keras.distribute import sidecar_evaluator\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/keras/distribute/sidecar_evaluator.py\", line 209\n",
            "    f'at {latest_checkpoint}. Retrying. '\n",
            "    ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##maxent-nltk.py"
      ],
      "metadata": {
        "id": "OrKycGVagOFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/maxent-nltk.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdEiKJURgOFp",
        "outputId": "8129fc96-109a-4145-9391-9ecaa0e0e49c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (1 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.565\n",
            "         Final          -0.61640        0.755\n",
            "  -1.010 andyhurleyday==True and label is '0'\n",
            "   1.000 twilightnewmoon==True and label is '0'\n",
            "   1.000 princecharming==True and label is '0'\n",
            "   1.000 notfeelingwell==True and label is '0'\n",
            "   1.000 hayfeversucks==True and label is '0'\n",
            "   1.000 halfbloodprince==True and label is '1'\n",
            "   1.000 alonee==True and label is '0'\n",
            "   1.000 ohhushfornewmoon==True and label is '1'\n",
            "   1.000 alltimelowweek==True and label is '1'\n",
            "   1.000 thaipbs==True and label is '1'\n",
            "Validation set accuracy:0.0000\n",
            "\n",
            "Predicting for test data\n",
            "\n",
            "Saved to maxent.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##decisiontree.py"
      ],
      "metadata": {
        "id": "2GMgG12TgORX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/decisiontree.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ8zfty4gORX",
        "outputId": "63ebf660-fdfc-40c4-f5c3-bae5899cae6b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6795/10000 = 67.9500 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##randomforest.py"
      ],
      "metadata": {
        "id": "nG2jyZmtgOXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/code/randomforest.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsp_a67xgOXp",
        "outputId": "f742ef4f-26c1-44b9-a039-f496da0492e7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7261/10000 = 72.6100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "\n",
        "def file_to_wordset(filename):\n",
        "    ''' Converts a file with a word per line to a Python set '''\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "\n",
        "def write_status(i, total):\n",
        "    ''' Writes status of a process to console '''\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (i, total))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {bigram:rank} of top N bigrams from a pickle\n",
        "    file which has a Counter object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of bigrams to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {bigram:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def split_data(tweets, validation_split=0.1):\n",
        "    \"\"\"Split the data into training and validation sets\n",
        "    Args:\n",
        "        tweets (list): list of tuples\n",
        "        validation_split (float, optional): validation split %\n",
        "    Returns:\n",
        "        (list, list): training-set, validation-set\n",
        "    \"\"\"\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]"
      ],
      "metadata": {
        "id": "roo3_fsDdMI2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##XGBoost.py"
      ],
      "metadata": {
        "id": "n3NiJJFadXbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/xgboost.py TRAIN = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBOvm2J7dQXI",
        "outputId": "9abaf08f-296c-4adb-f7ac-cafd04ad3213"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Twitter-Sentiment-Analysis-main/code/xgboost.py\", line 4, in <module>\n",
            "    from xgboost import XGBClassifier\n",
            "  File \"/content/Twitter-Sentiment-Analysis-main/code/xgboost.py\", line 4, in <module>\n",
            "    from xgboost import XGBClassifier\n",
            "ImportError: cannot import name XGBClassifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    xrange = range\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=400)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lla9qhkdkS3",
        "outputId": "9a1c9423-a3a2-4e88-ec8e-f2b62112426c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7668/10000 = 76.6800 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM.py"
      ],
      "metadata": {
        "id": "uOWE9wbyfiwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/svm.py TRAIN = True"
      ],
      "metadata": {
        "id": "OBeKDvsvfiwR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using SVM.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = svm.LinearSVC(C=0.1)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'svm.csv')\n",
        "        print ('\\nSaved to svm.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLu7QTe0mecI",
        "outputId": "59841a9a-ea85-4777-d0ec-f2c06e84d366"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7796/10000 = 77.9600 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##neuralnet.py"
      ],
      "metadata": {
        "id": "6K6gi-HBfy1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/neuralnet.py TRAIN = True"
      ],
      "metadata": {
        "id": "9m8ftj7bfy1L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Performs classification using an MLP/1-hidden-layer NN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = np.zeros((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_tweets):\n",
        "    correct, total = 0, len(val_tweets)\n",
        "    for val_set_X, val_set_y in extract_features(val_tweets, feat_type=FEAT_TYPE, test_file=False):\n",
        "        prediction = model.predict_on_batch(val_set_X)\n",
        "        prediction = np.round(prediction)\n",
        "        correct += np.sum(prediction == val_set_y[:, None])\n",
        "    return float(correct) / total\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    nb_epochs = 5\n",
        "    batch_size = 500\n",
        "    model = build_model()\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    best_val_acc = 0.0\n",
        "    for j in xrange(nb_epochs):\n",
        "        i = 1\n",
        "        for training_set_X, training_set_y in extract_features(train_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=False):\n",
        "            o = model.train_on_batch(training_set_X, training_set_y)\n",
        "            sys.stdout.write('\\rIteration %d/%d, loss:%.4f, acc:%.4f' %\n",
        "                             (i, n_train_batches, o[0], o[1]))\n",
        "            sys.stdout.flush()\n",
        "            i += 1\n",
        "        val_acc = evaluate_model(model, val_tweets)\n",
        "        print ('\\nEpoch: %d, val_acc:%.4f' % (j + 1, val_acc))\n",
        "        random.shuffle(train_tweets)\n",
        "        if val_acc > best_val_acc:\n",
        "            print ('Accuracy improved from %.4f to %.4f, saving model' % (best_val_acc, val_acc))\n",
        "            best_val_acc = val_acc\n",
        "            model.save('best_model.h5')\n",
        "    print ('Testing')\n",
        "    del train_tweets\n",
        "    del model\n",
        "    model = load_model('best_model.h5')\n",
        "    test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "    predictions = np.array([])\n",
        "    print ('Predicting batches')\n",
        "    i = 1\n",
        "    for test_set_X, _ in extract_features(test_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=True):\n",
        "        prediction = np.round(model.predict_on_batch(test_set_X).flatten())\n",
        "        predictions = np.concatenate((predictions, prediction))\n",
        "        write_status(i, n_test_batches)\n",
        "        i += 1\n",
        "    predictions = [(str(j), int(predictions[j]))\n",
        "                   for j in range(len(test_tweets))]\n",
        "    save_results_to_csv(predictions, '1layerneuralnet.csv')\n",
        "    print ('\\nSaved to 1layerneuralnet.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLwToHxgmfFZ",
        "outputId": "0e3d578c-c707-4198-857b-f31f27c99011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Iteration 180/180, loss:0.5178, acc:0.7460\n",
            "Epoch: 1, val_acc:0.7620\n",
            "Accuracy improved from 0.0000 to 0.7620, saving model\n",
            "Iteration 180/180, loss:0.4993, acc:0.7740\n",
            "Epoch: 2, val_acc:0.7684\n",
            "Accuracy improved from 0.7620 to 0.7684, saving model\n",
            "Iteration 180/180, loss:0.4202, acc:0.8020\n",
            "Epoch: 3, val_acc:0.7710\n",
            "Accuracy improved from 0.7684 to 0.7710, saving model\n",
            "Iteration 180/180, loss:0.4941, acc:0.7620\n",
            "Epoch: 4, val_acc:0.7709\n",
            "Iteration 180/180, loss:0.4660, acc:0.7960\n",
            "Epoch: 5, val_acc:0.7705\n",
            "Testing\n",
            "Generating feature vectors\n",
            "Processing 229586/300000"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##lstm.py"
      ],
      "metadata": {
        "id": "p8Ey4NAPfzUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/lstm.py TRAIN = True"
      ],
      "metadata": {
        "id": "iFsAsF_afzUp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Performs classification using LSTM network.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = './dataset/glove-seeds.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    print ('Looking for GLOVE vectors')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    print ('Found %d words in GLOVE' % found)\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
        "    for word, i in vocab.items():\n",
        "        glove_vector = glove_vectors.get(word)\n",
        "        if glove_vector is not None:\n",
        "            embedding_matrix[i] = glove_vector\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    if train:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(LSTM(128))\n",
        "        model.add(Dense(64))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        filepath = \"./models/lstm-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "        print (model.summary())\n",
        "        model.fit(tweets, labels, batch_size=128, epochs=5, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
        "    else:\n",
        "        model = load_model(sys.argv[1])\n",
        "        print (model.summary())\n",
        "        test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "        predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
        "        results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
        "        save_results_to_csv(results, 'lstm.csv')\n"
      ],
      "metadata": {
        "id": "B3vYPLudmfkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##cnn.py"
      ],
      "metadata": {
        "id": "iNX1exg6fzhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/cnn.py TRAIN = True"
      ],
      "metadata": {
        "id": "EmY8XTUrgcQR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, Flatten\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Performs classification using CNN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = './dataset/glove-seeds.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    \"\"\"\n",
        "    Extracts glove vectors from seed file only for words present in vocab.\n",
        "    \"\"\"\n",
        "    print ('Looking for GLOVE seeds')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    \"\"\"\n",
        "    Generates a feature vector for each tweet where each word is\n",
        "    represented by integer index based on rank in vocabulary.\n",
        "    \"\"\"\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"\n",
        "    Generates training X, y pairs.\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    # Create and embedding matrix\n",
        "    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
        "    # Seed it with GloVe vectors\n",
        "    for word, i in vocab.items():\n",
        "        glove_vector = glove_vectors.get(word)\n",
        "        if glove_vector is not None:\n",
        "            embedding_matrix[i] = glove_vector\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    if train:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(600))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        filepath = \"./models/4cnn-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "        model.fit(tweets, labels, batch_size=128, epochs=8, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
        "    else:\n",
        "        model = load_model(sys.argv[1])\n",
        "        print (model.summary())\n",
        "        test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "        predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
        "        results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
        "        save_results_to_csv(results, 'cnn.csv')\n"
      ],
      "metadata": {
        "id": "L4Qg_ukSfe9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##extract-cnn-feats.py"
      ],
      "metadata": {
        "id": "NfuNgGe_mhRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/extract-cnn-feats.py TRAIN = True"
      ],
      "metadata": {
        "id": "l9l33ikkmhRF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import load_model, Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Extracts dense vector features from penultimate layer of CNN model.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = './dataset/glove-seeds.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    print ('Looking for GLOVE seeds')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    model = load_model(sys.argv[1])\n",
        "    model = Model(model.layers[0].input, model.layers[-3].output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print (model.summary())\n",
        "    test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "    predictions = model.predict(test_tweets, batch_size=1024, verbose=1)\n",
        "    np.save('test-feats.npy', predictions)\n",
        "    predictions = model.predict(tweets, batch_size=1024, verbose=1)\n",
        "    np.save('train-feats.npy', predictions)\n",
        "    np.savetxt('train-labels.txt', labels)\n"
      ],
      "metadata": {
        "id": "zrW5g1lHrIxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##cnn-feats-svm.py"
      ],
      "metadata": {
        "id": "V-gUehfgsqnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/cnn-feats-svm.py TRAIN = True"
      ],
      "metadata": {
        "id": "mLufJOEZsqnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Performs SVM classification on features extracted from penultimate layer of CNN model.\n",
        "\n",
        "\n",
        "TRAIN_FEATURES_FILE = './train-feats.npy'\n",
        "TRAIN_LABELS_FILE = './train-labels.txt'\n",
        "TEST_FEATURES_FILE = './test-feats.npy'\n",
        "CLASSIFIER = 'SVM'\n",
        "MODEL_FILE = 'cnn-feats-%s.pkl' % CLASSIFIER\n",
        "TRAIN = True\n",
        "C = 1\n",
        "MAX_ITER = 1000\n",
        "\n",
        "if TRAIN:\n",
        "    X_train = np.load(TRAIN_FEATURES_FILE)\n",
        "    y_train = loadtxt(TRAIN_LABELS_FILE, dtype=float).astype(int)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
        "\n",
        "    print X_train.shape, y_train.shape, X_val.shape, y_val.shape\n",
        "\n",
        "    if CLASSIFIER == 'SVM':\n",
        "        model = svm.LinearSVC(C=C, verbose=1, max_iter=MAX_ITER)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    print model\n",
        "    del X_train\n",
        "    del y_train\n",
        "    with open(MODEL_FILE, 'wb') as mf:\n",
        "        pickle.dump(model, mf)\n",
        "    val_preds = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, val_preds)\n",
        "    print(\"Val Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "\n",
        "else:\n",
        "    with open(MODEL_FILE, 'rb') as mf:\n",
        "        model = pickle.load(mf)\n",
        "    X_test = np.load(TEST_FEATURES_FILE)\n",
        "    print X_test.shape\n",
        "    test_preds = model.predict(X_test)\n",
        "    results = zip(map(str, range(X_test.shape[0])), test_preds)\n",
        "    save_results_to_csv(results, 'cnn-feats-svm-linear-%.2f-%d.csv' % (C, MAX_ITER))\n"
      ],
      "metadata": {
        "id": "8Nff1-QQt7VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##majority-voting.py"
      ],
      "metadata": {
        "id": "0kzN6v2Zsrrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/majority-voting.py TRAIN = True"
      ],
      "metadata": {
        "id": "3Feb5-NNsrrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "# Takes majority vote of a number of CSV prediction files.\n",
        "\n",
        "NUM_PREDICTION_ROWS = 200000\n",
        "\n",
        "\n",
        "def main():\n",
        "    csvs = glob.glob('results/*.csv')\n",
        "    predictions = np.zeros((NUM_PREDICTION_ROWS, 2))\n",
        "    for csv in csvs:\n",
        "        with open(csv, 'r') as f:\n",
        "            lines = f.readlines()[1:]\n",
        "            current_preds = np.array([int(l.split(',')[1]) for l in lines])\n",
        "            predictions[range(NUM_PREDICTION_ROWS), current_preds] += 1\n",
        "    print (predictions[:50])\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    results = zip(map(str, range(NUM_PREDICTION_ROWS)), predictions)\n",
        "    save_results_to_csv(results, 'majority-voting.csv')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Gi2ex_0dt8KW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}