{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muchlisam17/Machine-Learning-Task-TelU/blob/main/Final%20Exam/Code%202/Final%20Exam%20%3A%20Code%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD7PTT1lJdcq",
        "outputId": "098a82c1-832c-478a-8113-426fe3d63b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Twitter-Sentiment-Analysis-main.zip\n",
            "   creating: Twitter-Sentiment-Analysis-main/.idea/\n",
            " extracting: Twitter-Sentiment-Analysis-main/.idea/.gitignore  \n",
            "   creating: Twitter-Sentiment-Analysis-main/.idea/inspectionProfiles/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/.idea/inspectionProfiles/profiles_settings.xml  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/.idea/misc.xml  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/.idea/modules.xml  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/.idea/Twitter-Sentiment-Analysis-main.iml  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/.idea/workspace.xml  \n",
            "   creating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/.gitignore  \n",
            "   creating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/baseline.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/cnn.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/cnn-feats-svm.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/decisiontree.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/extract-cnn-feats.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/logistic.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/lstm.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/majority-voting.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/maxent-nltk.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/naivebayes.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/neuralnet.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/preprocess.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/randomforest.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/stats.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/svm.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/utils.py  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/xgboost.py  \n",
            "   creating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/glove.twitter.27B.25d.txt  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/negative-words.txt  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/positive-words.txt  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test.csv  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train.csv  \n",
            "   creating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/docs/\n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/docs/Plots.ipynb  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/docs/Twitter Sentiment Analysis- (3).pdf  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/LICENSE  \n",
            "  inflating: Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/README.md  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/Twitter-Sentiment-Analysis-main.zip\n",
        "#meng-unzip file Twitter-Sentiment-Analysis-main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PfI5cnUETdrN",
        "outputId": "858ea1f8-581d-42b1-a073-3df8516f8e8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (1.16.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (0.20.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python2.7/dist-packages (from scikit-learn) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scikit-learn) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scipy) (1.16.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python2.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/73/205b5e7f8fe086ffe4165d984acb2c49fa3086f330f03099378753982d2e/tensorflow-2.1.0-cp27-cp27mu-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: scipy==1.2.2; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.2.2)\n",
            "Requirement already satisfied: wheel; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.11.2)\n",
            "Collecting keras-preprocessing>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.0.post1)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 48.7MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/56/4bcec5a8d9503a87e58e814c4e32ac2b32c37c685672c30bc8c54c6e478a/Keras_Applications-1.0.8.tar.gz (289kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: functools32>=3.2.3; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (3.2.3.post2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.7.1)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/c5/94a66686b86adfb3ef6f34837d0a7cc2efdc995bc39cad64a9b3e103f0d5/tensorboard-2.1.0-py2-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (1.1.10)\n",
            "Requirement already satisfied: mock>=2.0.0; python_version < \"3\" in /usr/local/lib/python2.7/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.8.0->tensorflow) (44.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.15.5)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/7a/1b3eb54caee1b8c73c2c3645f78a382eca4805a301a30c64a078e736e446/google_auth-1.35.0-py2.py3-none-any.whl (152kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0; python_version < \"3\"->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.5)\n",
            "Requirement already satisfied: rsa<4.6; python_version < \"3.6\" in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.6.16)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.5)\n",
            "Building wheels for collected packages: keras-applications, opt-einsum\n",
            "  Building wheel for keras-applications (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-applications: filename=Keras_Applications-1.0.8-cp27-none-any.whl size=50703 sha256=677ac7491987afbd6bf5fabc39b54a0b8a53abd65c8e1d48a71fa84ba86d5fc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/f2/5d/2689b5547f32c4e258c3b7ccbe7f1d0f2afbb84fb01e830792\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opt-einsum: filename=opt_einsum-2.3.2-cp27-none-any.whl size=49883 sha256=be73cdb3049cae360fb71e2bac6c0fd80d725f166efc88542e49a9d7e600d061\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built keras-applications opt-einsum\n",
            "\u001b[31mERROR: tensorboard 2.1.0 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-preprocessing, tensorflow-estimator, keras-applications, opt-einsum, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Found existing installation: google-auth 2.3.3\n",
            "    Uninstalling google-auth-2.3.3:\n",
            "      Successfully uninstalled google-auth-2.3.3\n",
            "  Found existing installation: google-auth-oauthlib 0.4.0\n",
            "    Uninstalling google-auth-oauthlib-0.4.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.0\n",
            "Successfully installed google-auth-1.35.0 google-auth-oauthlib-0.4.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 opt-einsum-2.3.2 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "keras_preprocessing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/49/7e10686647f741bd9c8918b0decdb94135b542fe372ca1100739b8529503/xgboost-0.82-py2.py3-none-manylinux1_x86_64.whl (114.0MB)\n",
            "\u001b[K     |████████████████████████████████| 114.0MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (from xgboost) (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from xgboost) (1.16.4)\n",
            "Installing collected packages: xgboost\n",
            "Successfully installed xgboost-0.82\n"
          ]
        }
      ],
      "source": [
        "!pip2 install numpy\n",
        "!pip2 install scikit-learn\n",
        "!pip2 install scipy\n",
        "!pip2 install nltk\n",
        "!pip2 install tensorflow\n",
        "!pip2 install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnF6P6xKasTv"
      },
      "source": [
        "##Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbkexZ2OUagn",
        "outputId": "fcad35ad-e48e-4850-d4d3-72840c790584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 100000/100000\n",
            "Saved processed tweets to: /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train.csv\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x57g8n7IbCze"
      },
      "source": [
        "##Preprocess Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79xSGxT9Y37X",
        "outputId": "56af3509-68b7-4b5f-fac4-3bce4308e74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 300000/300000\n",
            "Saved processed tweets to: /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/preprocess.py /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-6nUt0cbBDV"
      },
      "source": [
        "##Preprocess Running of stats.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7j53NyGbAwu",
        "outputId": "d37a66d3-7ca2-4801-d89e-4ea2c5a13d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 100000/100000\n",
            "Calculating frequency distribution\n",
            "Saved uni-frequency distribution to /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl\n",
            "Saved bi-frequency distribution to /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl\n",
            "\n",
            "[Analysis Statistics]\n",
            "Tweets => Total: 100000, Positive: 56462, Negative: 43538\n",
            "User Mentions => Total: 88955, Avg: 0.8895, Max: 12\n",
            "URLs => Total: 3599, Avg: 0.0360, Max: 4\n",
            "Emojis => Total: 1184, Positive: 997, Negative: 187, Avg: 0.0118, Max: 5\n",
            "Words => Total: 1192617, Unique: 50377, Avg: 11.9262, Max: 40, Min: 0\n",
            "Bigrams => Total: 1093149, Unique: 385105, Avg: 10.9315\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/stats.py /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EKdweOMbfz4"
      },
      "source": [
        "##baseline.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofM_CdCpb9tC",
        "outputId": "11dc6a98-f6b1-4019-b897-785ea4230217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct = 65.35%\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/baseline.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tJX8YIhcpHF"
      },
      "source": [
        "##naivebayes.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw-Bs6PVdA7i",
        "outputId": "61d1bf64-7f6c-4bb9-bd2f-bb2bd1ff09d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7802/10000 = 78.0200 %\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/naivebayes.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R5hsYITgN6i"
      },
      "source": [
        "##logistic.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqbqcVs5gN6j",
        "outputId": "1da610fa-96b4-48fc-fab3-e427ae0126aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-01-25 02:48:41.173466: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2022-01-25 02:48:41.173580: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2022-01-25 02:48:41.173598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/logistic.py\", line 1, in <module>\n",
            "    from keras.models import Sequential, load_model\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/keras/__init__.py\", line 22, in <module>\n",
            "    from keras import distribute\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/keras/distribute/__init__.py\", line 18, in <module>\n",
            "    from keras.distribute import sidecar_evaluator\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/keras/distribute/sidecar_evaluator.py\", line 209\n",
            "    f'at {latest_checkpoint}. Retrying. '\n",
            "    ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/logistic.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrKycGVagOFp"
      },
      "source": [
        "##maxent-nltk.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdEiKJURgOFp",
        "outputId": "04b4e4eb-5907-4e09-f731-7be4def6ab3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ==> Training (1 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.564\n",
            "         Final          -0.61640        0.756\n",
            "  -1.004 andyhurleyday==True and label is '0'\n",
            "   1.000 thestreetforce==True and label is '1'\n",
            "   1.000 alltimelowweek==True and label is '1'\n",
            "   1.000 alonee==True and label is '0'\n",
            "   1.000 hitech==True and label is '1'\n",
            "   1.000 thaipbs==True and label is '1'\n",
            "   1.000 recess==True and label is '1'\n",
            "   1.000 ephesians==True and label is '1'\n",
            "   1.000 atthepool==True and label is '1'\n",
            "   1.000 hayfeversucks==True and label is '0'\n",
            "Validation set accuracy:0.0000\n",
            "\n",
            "Predicting for test data\n",
            "\n",
            "Saved to maxent.csv\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/maxent-nltk.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GMgG12TgORX"
      },
      "source": [
        "##decisiontree.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ8zfty4gORX",
        "outputId": "4c4730f0-aa30-46fb-f6f2-034f08ac60d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6853/10000 = 68.5300 %\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/decisiontree.py TRAIN = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG2jyZmtgOXp"
      },
      "source": [
        "##randomforest.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsp_a67xgOXp",
        "outputId": "61b2bf05-5e3d-4886-c742-9172ce32783c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7227/10000 = 72.2700 %\n"
          ]
        }
      ],
      "source": [
        "!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/randomforest.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roo3_fsDdMI2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "\n",
        "def file_to_wordset(filename):\n",
        "    ''' Converts a file with a word per line to a Python set '''\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "\n",
        "def write_status(i, total):\n",
        "    ''' Writes status of a process to console '''\n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (i, total))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {word:rank} of top N words from a pickle\n",
        "    file which has a nltk FreqDist object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of words to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {word:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
        "    \"\"\"\n",
        "    Returns a dictionary of form {bigram:rank} of top N bigrams from a pickle\n",
        "    file which has a Counter object generated by stats.py\n",
        "    Args:\n",
        "        pkl_file_name (str): Name of pickle file\n",
        "        N (int): The number of bigrams to get\n",
        "        shift: amount to shift the rank from 0.\n",
        "    Returns:\n",
        "        dict: Of form {bigram:rank}\n",
        "    \"\"\"\n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def split_data(tweets, validation_split=0.1):\n",
        "    \"\"\"Split the data into training and validation sets\n",
        "    Args:\n",
        "        tweets (list): list of tuples\n",
        "        validation_split (float, optional): validation split %\n",
        "    Returns:\n",
        "        (list, list): training-set, validation-set\n",
        "    \"\"\"\n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3NiJJFadXbG"
      },
      "source": [
        "##XGBoost.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBOvm2J7dQXI",
        "outputId": "779381aa-231c-4063-8caf-a6ba1703a665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/xgboost.py\", line 4, in <module>\n",
            "    from xgboost import XGBClassifier\n",
            "  File \"/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/xgboost.py\", line 4, in <module>\n",
            "    from xgboost import XGBClassifier\n",
            "ImportError: cannot import name XGBClassifier\n"
          ]
        }
      ],
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/xgboost.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lla9qhkdkS3",
        "outputId": "04c0887e-96ec-4678-ffe9-4e7ea88e619d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7719/10000 = 77.1900 %\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using XGBoost.\n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    xrange = range\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=400)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOWE9wbyfiwQ"
      },
      "source": [
        "##SVM.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBeKDvsvfiwR"
      },
      "outputs": [],
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/svm.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLu7QTe0mecI",
        "outputId": "cb0da8ef-b2f4-4426-cadb-ad55742ccbfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7880/10000 = 78.8000 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# Performs classification using SVM.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = svm.LinearSVC(C=0.1)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, 'svm.csv')\n",
        "        print ('\\nSaved to svm.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K6gi-HBfy1K"
      },
      "source": [
        "##neuralnet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m8ftj7bfy1L"
      },
      "outputs": [],
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/neuralnet.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLwToHxgmfFZ",
        "outputId": "41ceba7c-2714-4b87-dc5b-04cea7370a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Iteration 180/180, loss:0.5400, acc:0.7360\n",
            "Epoch: 1, val_acc:0.7639\n",
            "Accuracy improved from 0.0000 to 0.7639, saving model\n",
            "Iteration 180/180, loss:0.4719, acc:0.7960\n",
            "Epoch: 2, val_acc:0.7689\n",
            "Accuracy improved from 0.7639 to 0.7689, saving model\n",
            "Iteration 180/180, loss:0.4368, acc:0.7980\n",
            "Epoch: 3, val_acc:0.7695\n",
            "Accuracy improved from 0.7689 to 0.7695, saving model\n",
            "Iteration 180/180, loss:0.4445, acc:0.7980\n",
            "Epoch: 4, val_acc:0.7683\n",
            "Iteration 180/180, loss:0.4182, acc:0.8120\n",
            "Epoch: 5, val_acc:0.7668\n",
            "Testing\n",
            "Generating feature vectors\n",
            "Processing 300000/300000\n",
            "\n",
            "Predicting batches\n",
            "Processing 600/600\n",
            "Saved to 1layerneuralnet.csv\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Performs classification using an MLP/1-hidden-layer NN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = np.zeros((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_tweets):\n",
        "    correct, total = 0, len(val_tweets)\n",
        "    for val_set_X, val_set_y in extract_features(val_tweets, feat_type=FEAT_TYPE, test_file=False):\n",
        "        prediction = model.predict_on_batch(val_set_X)\n",
        "        prediction = np.round(prediction)\n",
        "        correct += np.sum(prediction == val_set_y[:, None])\n",
        "    return float(correct) / total\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    nb_epochs = 5\n",
        "    batch_size = 500\n",
        "    model = build_model()\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    best_val_acc = 0.0\n",
        "    for j in xrange(nb_epochs):\n",
        "        i = 1\n",
        "        for training_set_X, training_set_y in extract_features(train_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=False):\n",
        "            o = model.train_on_batch(training_set_X, training_set_y)\n",
        "            sys.stdout.write('\\rIteration %d/%d, loss:%.4f, acc:%.4f' %\n",
        "                             (i, n_train_batches, o[0], o[1]))\n",
        "            sys.stdout.flush()\n",
        "            i += 1\n",
        "        val_acc = evaluate_model(model, val_tweets)\n",
        "        print ('\\nEpoch: %d, val_acc:%.4f' % (j + 1, val_acc))\n",
        "        random.shuffle(train_tweets)\n",
        "        if val_acc > best_val_acc:\n",
        "            print ('Accuracy improved from %.4f to %.4f, saving model' % (best_val_acc, val_acc))\n",
        "            best_val_acc = val_acc\n",
        "            model.save('best_model.h5')\n",
        "    print ('Testing')\n",
        "    del train_tweets\n",
        "    del model\n",
        "    model = load_model('best_model.h5')\n",
        "    test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "    predictions = np.array([])\n",
        "    print ('Predicting batches')\n",
        "    i = 1\n",
        "    for test_set_X, _ in extract_features(test_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=True):\n",
        "        prediction = np.round(model.predict_on_batch(test_set_X).flatten())\n",
        "        predictions = np.concatenate((predictions, prediction))\n",
        "        write_status(i, n_test_batches)\n",
        "        i += 1\n",
        "    predictions = [(str(j), int(predictions[j]))\n",
        "                   for j in range(len(test_tweets))]\n",
        "    save_results_to_csv(predictions, '1layerneuralnet.csv')\n",
        "    print ('\\nSaved to 1layerneuralnet.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Ey4NAPfzUp"
      },
      "source": [
        "##lstm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFsAsF_afzUp"
      },
      "outputs": [],
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/lstm.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "B3vYPLudmfkD",
        "outputId": "6bf5231f-f1b7-46b6-be25-a119963af9c6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8f91abc13cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_n_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFREQ_DIST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mglove_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PROCESSED_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'top_n_words' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import LSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Performs classification using LSTM network.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/glove.twitter.27B.25d.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    print ('Looking for GLOVE vectors')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    print ('Found %d words in GLOVE' % found)\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
        "    for word, i in vocab.items():\n",
        "        glove_vector = glove_vectors.get(word)\n",
        "        if glove_vector is not None:\n",
        "            embedding_matrix[i] = glove_vector\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    if train:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(LSTM(128))\n",
        "        model.add(Dense(64))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        filepath = \"./models/lstm-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "        print (model.summary())\n",
        "        model.fit(tweets, labels, batch_size=128, epochs=5, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
        "    else:\n",
        "        model = load_model(sys.argv[1])\n",
        "        print (model.summary())\n",
        "        test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "        predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
        "        results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
        "        save_results_to_csv(results, 'lstm.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNX1exg6fzhe"
      },
      "source": [
        "##cnn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EmY8XTUrgcQR"
      },
      "outputs": [],
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/cnn.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "L4Qg_ukSfe9c",
        "outputId": "310a4d2f-3b5e-468d-afbe-2902c63bbedf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-cf74a0a0737b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_n_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFREQ_DIST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mglove_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PROCESSED_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'top_n_words' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, Flatten\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Performs classification using CNN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/glove.twitter.27B.25d.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    \"\"\"\n",
        "    Extracts glove vectors from seed file only for words present in vocab.\n",
        "    \"\"\"\n",
        "    print ('Looking for GLOVE seeds')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    \"\"\"\n",
        "    Generates a feature vector for each tweet where each word is\n",
        "    represented by integer index based on rank in vocabulary.\n",
        "    \"\"\"\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"\n",
        "    Generates training X, y pairs.\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    # Create and embedding matrix\n",
        "    embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
        "    # Seed it with GloVe vectors\n",
        "    for word, i in vocab.items():\n",
        "        glove_vector = glove_vectors.get(word)\n",
        "        if glove_vector is not None:\n",
        "            embedding_matrix[i] = glove_vector\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    if train:\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(300, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(150, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Conv1D(75, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(600))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(1))\n",
        "        model.add(Activation('sigmoid'))\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        filepath = \"./models/4cnn-{epoch:02d}-{loss:0.3f}-{acc:0.3f}-{val_loss:0.3f}-{val_acc:0.3f}.hdf5\"\n",
        "        checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
        "        model.fit(tweets, labels, batch_size=128, epochs=8, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])\n",
        "    else:\n",
        "        model = load_model(sys.argv[1])\n",
        "        print (model.summary())\n",
        "        test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "        predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
        "        results = zip(map(str, range(len(test_tweets))), np.round(predictions[:, 0]).astype(int))\n",
        "        save_results_to_csv(results, 'cnn.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfuNgGe_mhRD"
      },
      "source": [
        "##extract-cnn-feats.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l9l33ikkmhRF"
      },
      "outputs": [],
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/extract-cnn-feats.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "zrW5g1lHrIxS",
        "outputId": "3ae2b6d6-8dc8-4113-feb9-2321dcfa82f8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-db3a6d724498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_n_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFREQ_DIST_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mglove_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PROCESSED_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'top_n_words' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "from keras.models import load_model, Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Extracts dense vector features from penultimate layer of CNN model.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/test-processed.csv'\n",
        "GLOVE_FILE = '/content/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/glove.twitter.27B.25d.txt'\n",
        "dim = 200\n",
        "\n",
        "\n",
        "def get_glove_vectors(vocab):\n",
        "    print ('Looking for GLOVE seeds')\n",
        "    glove_vectors = {}\n",
        "    found = 0\n",
        "    with open(GLOVE_FILE, 'r') as glove_file:\n",
        "        for i, line in enumerate(glove_file):\n",
        "            write_status(i + 1, 0)\n",
        "            tokens = line.strip().split()\n",
        "            word = tokens[0]\n",
        "            if vocab.get(word):\n",
        "                vector = [float(e) for e in tokens[1:]]\n",
        "                glove_vectors[word] = np.array(vector)\n",
        "                found += 1\n",
        "    print ('\\n')\n",
        "    return glove_vectors\n",
        "\n",
        "\n",
        "def get_feature_vector(tweet):\n",
        "    words = tweet.split()\n",
        "    feature_vector = []\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        if vocab.get(word) is not None:\n",
        "            feature_vector.append(vocab.get(word))\n",
        "    if len(words) >= 1:\n",
        "        if vocab.get(words[-1]) is not None:\n",
        "            feature_vector.append(vocab.get(words[-1]))\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append(feature_vector)\n",
        "            else:\n",
        "                tweets.append(feature_vector)\n",
        "                labels.append(int(sentiment))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets, np.array(labels)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = len(sys.argv) == 1\n",
        "    np.random.seed(1337)\n",
        "    vocab_size = 90000\n",
        "    batch_size = 500\n",
        "    max_length = 40\n",
        "    filters = 600\n",
        "    kernel_size = 3\n",
        "    vocab = top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
        "    glove_vectors = get_glove_vectors(vocab)\n",
        "    tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
        "    shuffled_indices = np.random.permutation(tweets.shape[0])\n",
        "    tweets = tweets[shuffled_indices]\n",
        "    labels = labels[shuffled_indices]\n",
        "    model = load_model(sys.argv[1])\n",
        "    model = Model(model.layers[0].input, model.layers[-3].output)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    print (model.summary())\n",
        "    test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
        "    predictions = model.predict(test_tweets, batch_size=1024, verbose=1)\n",
        "    np.save('test-feats.npy', predictions)\n",
        "    predictions = model.predict(tweets, batch_size=1024, verbose=1)\n",
        "    np.save('train-feats.npy', predictions)\n",
        "    np.savetxt('train-labels.txt', labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-gUehfgsqnt"
      },
      "source": [
        "##cnn-feats-svm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLufJOEZsqnx"
      },
      "outputs": [],
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/cnn-feats-svm.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8Nff1-QQt7VA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "6b235171-8548-48e5-8e38-6a17fde0198c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-9b131c6aa0eb>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    print X_train.shape, y_train.shape, X_val.shape, y_val.shape\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)?\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from numpy import loadtxt\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Performs SVM classification on features extracted from penultimate layer of CNN model.\n",
        "\n",
        "\n",
        "TRAIN_FEATURES_FILE = './train-feats.npy'\n",
        "TRAIN_LABELS_FILE = './train-labels.txt'\n",
        "TEST_FEATURES_FILE = './test-feats.npy'\n",
        "CLASSIFIER = 'SVM'\n",
        "MODEL_FILE = 'cnn-feats-%s.pkl' % CLASSIFIER\n",
        "TRAIN = True\n",
        "C = 1\n",
        "MAX_ITER = 1000\n",
        "\n",
        "if TRAIN:\n",
        "    X_train = np.load(TRAIN_FEATURES_FILE)\n",
        "    y_train = loadtxt(TRAIN_LABELS_FILE, dtype=float).astype(int)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)\n",
        "\n",
        "    print X_train.shape, y_train.shape, X_val.shape, y_val.shape\n",
        "\n",
        "    if CLASSIFIER == 'SVM':\n",
        "        model = svm.LinearSVC(C=C, verbose=1, max_iter=MAX_ITER)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    print model\n",
        "    del X_train\n",
        "    del y_train\n",
        "    with open(MODEL_FILE, 'wb') as mf:\n",
        "        pickle.dump(model, mf)\n",
        "    val_preds = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, val_preds)\n",
        "    print(\"Val Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "\n",
        "else:\n",
        "    with open(MODEL_FILE, 'rb') as mf:\n",
        "        model = pickle.load(mf)\n",
        "    X_test = np.load(TEST_FEATURES_FILE)\n",
        "    print X_test.shape\n",
        "    test_preds = model.predict(X_test)\n",
        "    results = zip(map(str, range(X_test.shape[0])), test_preds)\n",
        "    save_results_to_csv(results, 'cnn-feats-svm-linear-%.2f-%d.csv' % (C, MAX_ITER))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kzN6v2Zsrrc"
      },
      "source": [
        "##majority-voting.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Feb5-NNsrrd"
      },
      "outputs": [],
      "source": [
        "#!python2 /content/Twitter-Sentiment-Analysis-main/code/majority-voting.py TRAIN = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gi2ex_0dt8KW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5664e8ea-a454-41b2-f208-135da67d8bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9b104b2d3ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-9b104b2d3ee2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_PREDICTION_ROWS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0msave_results_to_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'majority-voting.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_results_to_csv' is not defined"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "# Takes majority vote of a number of CSV prediction files.\n",
        "\n",
        "NUM_PREDICTION_ROWS = 200000\n",
        "\n",
        "\n",
        "def main():\n",
        "    csvs = glob.glob('results/*.csv')\n",
        "    predictions = np.zeros((NUM_PREDICTION_ROWS, 2))\n",
        "    for csv in csvs:\n",
        "        with open(csv, 'r') as f:\n",
        "            lines = f.readlines()[1:]\n",
        "            current_preds = np.array([int(l.split(',')[1]) for l in lines])\n",
        "            predictions[range(NUM_PREDICTION_ROWS), current_preds] += 1\n",
        "    print (predictions[:50])\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    results = zip(map(str, range(NUM_PREDICTION_ROWS)), predictions)\n",
        "    save_results_to_csv(results, 'majority-voting.csv')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final Exam : Code 2",
      "provenance": [],
      "mount_file_id": "1jWlsVZBO4UzErIwmO8NMvcYBvSRCNq_z",
      "authorship_tag": "ABX9TyMgDQFu/7xO6nkyBnqnkAHR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}